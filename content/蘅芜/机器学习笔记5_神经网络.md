---
date: 2015-03-03
description: ""
tags: ["机器学习","有监督学习","神经网络"]
title: "机器学习笔记5 神经网络"
topics: []
draft: true
---
Andrew Ng cs229 Machine Learning 笔记

# 神经网络

## 非线性假设

在特征变量数较大的情况下，采用线性回归会很难处理，比如我的数据集有3个特征变量，想要在假设中引入所有特征变量的平方项：

<div>
$$\begin{align*}
& g(\theta_0 + \theta_1x_1^2 + \theta_2x_1x_2 + \theta_3x_1x_3 \newline
& + \theta_4x_2^2 + \theta_5x_2x_3 \newline
& + \theta_6x_3^2 )
\end{align*}$$
</div>

共有6个特征，采用允许重复的组合公式计算$\frac{(n+r-1)!}{r!(n-1)!}$，共有$\frac{(3 + 2 - 1)!}{(2!\cdot (3-1)!)} = 6$种特征变量的组合。对于100个特征变量，则共有$\frac{(100 + 2 - 1)!}{(2\cdot (100-1)!)} = 5050$个新的特征变量。

可以大致估计特征变量的平方项组合个数的增长速度为$\mathcal{O}(n^2/2)$，立方项的组合个数的增长为$\mathcal{O}(n^3)。这些增长都十分陡峭，让实际问题变得很棘手。

在变量假设十分复杂的情况下，神经网络提供了另一种机器学习算法。

# 神经元和大脑

神经网络是对大脑工作方式的一种简单模拟。有证据表明，大脑对所有的功能（如视觉，触觉，听觉等）都采用了一种“学习算法”。将听觉皮层和视觉神经连接到一起，听觉皮层可以学会“看见”。这种理论叫作“neuroplasticity”，有很多例子和实验验证。

# 模型表达

简单来说，每个神经元都有输入（树突dendrites）和输出（轴突axons）。在模型中，输入就是我们的特征变量，输出就是模型假设的结果。

在神经网络中，分类问题通常采用logistic函数，也叫做sigmoid激活函数(sigmoid activation function)。$\theta$参数有时也被称为权重(weights)

{{% img src="/media/neuron_network.tiff" alt="hugo-server-1" %}}


